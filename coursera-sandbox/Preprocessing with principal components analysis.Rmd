---
title: "Principal components analysis"
output: 
  html_document: 
    number_sections: yes
    toc: yes
---
#Covariates with principal components analysis

# Head
```{r, message=FALSE, warning=FALSE}
#clear
rm(list=ls())
#packages
library(caret)
library(ISLR)
library(dplyr)
library(splines)
library(kernlab)

```
# Data
```{r}
data(spam) 
colnames(spam)
```
## Training and testing
```{r}
inTrain <- createDataPartition(y=spam$type,list = FALSE,p = .75)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
```

# Correlated predictors 

```{r}
M <- abs(cor(training[,-58]))
colnames(M)
diag(M) <- 0
which(M > 0.8, arr.ind = T) 
```
```{r}
par(mfrow=c(1,3))
names(spam)[c(34,40)]
plot(spam[,34],spam[,40])
names(spam)[c(32,40)]
plot(spam[,32],spam[,40])
names(spam)[c(32,34)]
plot(spam[,32],spam[,34])

```  

Including correlation predictors not a good idea. A weighted combination might be better. We should pick the combination that captures the most information possible. 

Benefits:

* Reduced number of predictors
* Reduced noise due to averaging

```{r}
X <- 0.71*training$num415 + 0.71*training$num857
Y <- 0.71*training$num415 - 0.71*training$num857
plot(X,Y)

```

Plot shows that most of the variability is on the x axis, and that the data is clustered at y = 0. This means we should use the sum (X) as the predictor because it keeps more information. 

## Solutions

SVD and PCA

```{r}
smallSpam <- spam[,c(34,32)]
prComp <- prcomp(smallSpam)
plot(prComp$x[,1],prComp$x[,2])

```

Using principal components, you can sumarise more than 2 variables. 
```{r}
prComp$rotation
```

## Principal components on the whole data set

```{r}
typeColour <- ((spam$type=="spam")*1+1)
prComp <- prcomp(log10(spam[,-58]+1)) # added the log 10 and plus one to make the data look more normal because some of the variables are skewed. Often need to do this for principal components analysis. 
plot(prComp$x[,1],prComp$x[,2],col=typeColour,xlab="PC1",ylab="PC2")

```
  
Principal component 1 is a combination of other variables and explains the most variation in the data. PC2 explains the second most, etc. 

In PC1 there is more separation between spam and not spam. 

## PCA with caret
```{r}
preProc <- preProcess(log10(spam[,-58]+1),method = "pca", pcaComp = 2)
spamPC <- predict(preProc, log10(spam[,-58]+1))
plot(spamPC[,1],spamPC[,2],col=typeColour)
```

```{r, message=FALSE, warning=FALSE}
# calculate pca data for train and test
preProc <- preProcess(log10(training[,-58]+1),method="pca",pcaComp=2)
trainPC <- predict(preProc, log10(training[,-58]+1))
modelFit <- train(x=trainPC, y = training$type ,method="glm")
# used only the PC data as x vars
testPC <- predict(preProc, log10(testing[,-58]+1)) 
confusionMatrix(testing$type, predict(modelFit, testPC))

```
```{r}
#modelFit <- train(y=training$type, method="glm", preProcess="pca",x=training)
#confusionMatrix(testing$type,predict(modelFit,testing))
```

